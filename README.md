# Mcrosft_Fabric_End_to_End-Project-PySpark-Notebook-Data-Pipeline

This project demonstrates the end-to-end process of building a modern data lakehouse and analytics solution using Microsoft Fabric, Python, and Power BI. 

It showcases how raw data from the USGS Earthquake API can be ingested, processed through the medallion architecture (Bronze, Silver, and Gold layers), and ultimately transformed into actionable insights with interactive reports.

## The key stages covered include:

1. Creating a Fabric Workspace and Data Lakehouse for centralized storage and management.

2. Exploring the USGS Earthquake API and fetching seismic data using Python Requests.

3.  Bronze Layer Processing Notebook for raw data ingestion and schema alignment.

4. Silver Layer Processing Notebook for data cleaning, normalization, and enrichment.

5. Gold Layer Processing Notebook for business-ready transformations and analytics.

6. Updating the Default Semantic Model & Building Power BI Reports to visualize earthquake trends and insights.

7. Implementing a Data Factory Pipeline to orchestrate the entire workflow seamlessly.
